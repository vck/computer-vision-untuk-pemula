{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Module 6: Specialized Topics and Applications\n",
    "\n",
    "## Learning Objectives\n",
    "By the end of this module, you should be able to:\n",
    "- Process and analyze video sequences\n",
    "- Work with 3D data and point clouds\n",
    "- Apply computer vision to specialized domains\n",
    "- Implement real-world applications\n",
    "\n",
    "## Topics Covered\n",
    "- Video analysis and tracking\n",
    "- 3D computer vision and point clouds\n",
    "- Medical image analysis\n",
    "- Autonomous vehicles and robotics vision\n",
    "- Augmented reality applications"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1. Video Analysis and Tracking\n",
    "\n",
    "### Motion Detection\n",
    "\n",
    "Motion detection is a fundamental technique in video analysis."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import cv2\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "# Conceptual overview of motion detection\n",
    "fig, ax = plt.subplots(1, 1, figsize=(12, 8))\n",
    "ax.text(0.5, 0.9, 'Motion Detection Pipeline', ha='center', va='center', fontsize=16, transform=ax.transAxes)\n",
    "\n",
    "# Steps\n",
    "steps = [\n",
    "    (0.1, 0.7, 'Input Video\\nFrame'),\n",
    "    (0.3, 0.7, 'Background\\nSubtraction'),\n",
    "    (0.5, 0.7, 'Thresholding'),\n",
    "    (0.7, 0.7, 'Morphological\\nOperations'),\n",
    "    (0.9, 0.7, 'Contour\\nDetection')\n",
    "]\n",
    "\n",
    "# Draw steps\n",
    "for i, (x, y, text) in enumerate(steps):\n",
    "    ax.text(x, y, text, ha='center', va='center', fontsize=10, transform=ax.transAxes,\n",
    "            bbox=dict(boxstyle=\"round,pad=0.3\", facecolor=\"lightblue\"))\n",
    "    if i > 0:\n",
    "        ax.annotate('', xy=(x, y), xytext=(steps[i-1][0], steps[i-1][1]), arrowprops=dict(arrowstyle=\"->\", lw=2))\n",
    "\n",
    "# Detailed explanation\n",
    "ax.text(0.3, 0.5, 'Background Subtraction:\\n- Running average\\n- Mixture of Gaussians\\n- Frame differencing', \n",
    "        ha='center', va='center', fontsize=10, transform=ax.transAxes,\n",
    "        bbox=dict(boxstyle=\"round,pad=0.3\", facecolor=\"lightgreen\"))\n",
    "\n",
    "ax.text(0.7, 0.5, 'Applications:\\n- Surveillance\\n- Traffic monitoring\\n- Activity recognition', \n",
    "        ha='center', va='center', fontsize=10, transform=ax.transAxes,\n",
    "        bbox=dict(boxstyle=\"round,pad=0.3\", facecolor=\"lightcoral\"))\n",
    "\n",
    "ax.set_title('Motion Detection Concept')\n",
    "ax.axis('off')\n",
    "plt.tight_layout()\n",
    "plt.show()\n",
    "\n",
    "print(\"Motion Detection Techniques:\")\n",
    "print(\"1. Frame Differencing - Simple but sensitive to noise\")\n",
    "print(\"2. Background Subtraction - More robust for static scenes\")\n",
    "print(\"3. Optical Flow - Tracks motion vectors\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Object Tracking\n",
    "\n",
    "Object tracking follows objects across video frames."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Object tracking overview\n",
    "fig, ax = plt.subplots(1, 1, figsize=(14, 10))\n",
    "ax.text(0.5, 0.95, 'Object Tracking Pipeline', ha='center', va='center', fontsize=16, transform=ax.transAxes)\n",
    "\n",
    "# Tracking pipeline\n",
    "pipeline = [
    "    (0.1, 0.8, 'Object\\nDetection'),\n",
    "    (0.25, 0.8, 'Feature\\nExtraction'),\n",
    "    (0.4, 0.8, 'Data\\nAssociation'),\n",
    "    (0.55, 0.8, 'Track\\nManagement'),\n",
    "    (0.7, 0.8, 'Trajectory\\nPrediction'),\n",
    "    (0.85, 0.8, 'Visualization')\n",
    "]\n",
    "\n",
    "# Draw pipeline\n",
    "for i, (x, y, text) in enumerate(pipeline):\n",
    "    ax.text(x, y, text, ha='center', va='center', fontsize=9, transform=ax.transAxes,\n",
    "            bbox=dict(boxstyle=\"round,pad=0.3\", facecolor=\"lightblue\"))\n",
    "    if i > 0:\n",
    "        ax.annotate('', xy=(x, y), xytext=(pipeline[i-1][0], pipeline[i-1][1]), arrowprops=dict(arrowstyle=\"->\", lw=2))\n",
    "\n",
    "# Tracking algorithms\n",
    "algorithms = [
    "    (0.2, 0.6, 'Traditional:\\n- KLT Tracker\\n- Mean Shift\\n- CAMShift'),\n",
    "    (0.5, 0.6, 'Deep Learning:\\n- SORT\\n- DeepSORT\\n- FairMOT'),\n",
    "    (0.8, 0.6, 'Applications:\\n- Surveillance\\n- Sports analysis\\n- Autonomous vehicles')\n",
    "]\n",
    "\n",
    "# Draw algorithms\n",
    "for x, y, text in algorithms:\n",
    "    ax.text(x, y, text, ha='center', va='center', fontsize=9, transform=ax.transAxes,\n",
    "            bbox=dict(boxstyle=\"round,pad=0.3\", facecolor=\"lightgreen\"))\n",
    "\n",
    "# Challenges\n",
    "ax.text(0.5, 0.4, 'Tracking Challenges:\\n- Occlusion\\n- Illumination changes\\n- Scale variations\\n- Fast motion\\n- Similar objects', \n",
    "        ha='center', va='center', fontsize=10, transform=ax.transAxes,\n",
    "        bbox=dict(boxstyle=\"round,pad=0.3\", facecolor=\"lightcoral\"))\n",
    "\n",
    "ax.set_title('Object Tracking Architecture')\n",
    "ax.axis('off')\n",
    "plt.tight_layout()\n",
    "plt.show()\n",
    "\n",
    "print(\"Popular Tracking Algorithms:\")\n",
    "print(\"1. KLT (Kanade-Lucas-Tomasi) - Feature point tracking\")\n",
    "print(\"2. Mean Shift/CAMShift - Color-based tracking\")\n",
    "print(\"3. SORT/DeepSORT - Detection-based tracking with Kalman filters\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2. 3D Computer Vision and Point Clouds\n",
    "\n",
    "### Point Cloud Processing\n",
    "\n",
    "Point clouds represent 3D data as sets of points in space."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Point cloud concept\n",
    "fig = plt.figure(figsize=(12, 8))\n",
    "ax = fig.add_subplot(111, projection='3d')\n",
    "\n",
    "# Generate sample point cloud data\n",
    "np.random.seed(42)\n",
    "n_points = 1000\n",
    "x = np.random.randn(n_points)\n",
    "y = np.random.randn(n_points)\n",
    "z = np.random.randn(n_points)\n",
    "\n",
    "# Create different colored regions\n",
    "colors = np.zeros((n_points, 3))\n",
    "dist = np.sqrt(x**2 + y**2 + z**2)\n",
    "colors[:, 0] = dist / np.max(dist)  # Red channel\n",
    "colors[:, 1] = 0.5  # Green channel\n",
    "colors[:, 2] = 1 - dist / np.max(dist)  # Blue channel\n",
    "\n",
    "# Plot point cloud\n",
    "ax.scatter(x, y, z, c=colors, s=10, alpha=0.7)\n",
    "ax.set_xlabel('X')\n",
    "ax.set_ylabel('Y')\n",
    "ax.set_zlabel('Z')\n",
    "ax.set_title('Sample Point Cloud Visualization')\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()\n",
    "\n",
    "# Point cloud processing pipeline\n",
    "fig, ax = plt.subplots(1, 1, figsize=(14, 8))\n",
    "ax.text(0.5, 0.9, 'Point Cloud Processing Pipeline', ha='center', va='center', fontsize=16, transform=ax.transAxes)\n",
    "\n",
    "# Steps\n",
    "steps = [
    "    (0.1, 0.7, 'Data\\nAcquisition'),\n",
    "    (0.25, 0.7, 'Preprocessing\\n(Filtering,\\nDenoising)'),\n",
    "    (0.4, 0.7, 'Registration\\n(Alignment)'),\n",
    "    (0.55, 0.7, 'Segmentation\\n(Clustering)'),\n",
    "    (0.7, 0.7, 'Feature\\nExtraction'),\n",
    "    (0.85, 0.7, 'Classification\\n/Recognition')\n",
    "]\n",
    "\n",
    "# Draw steps\n",
    "for i, (x, y, text) in enumerate(steps):\n",
    "    ax.text(x, y, text, ha='center', va='center', fontsize=9, transform=ax.transAxes,\n",
    "            bbox=dict(boxstyle=\"round,pad=0.3\", facecolor=\"lightblue\"))\n",
    "    if i > 0:\n",
    "        ax.annotate('', xy=(x, y), xytext=(steps[i-1][0], steps[i-1][1]), arrowprops=dict(arrowstyle=\"->\", lw=2))\n",
    "\n",
    "# Technologies\n",
    "ax.text(0.2, 0.5, 'Acquisition:\\n- LiDAR\\n- Stereo cameras\\n- Structured light', \n",
    "        ha='center', va='center', fontsize=9, transform=ax.transAxes,\n",
    "        bbox=dict(boxstyle=\"round,pad=0.3\", facecolor=\"lightgreen\"))\n",
    "\n",
    "ax.text(0.5, 0.5, 'Processing:\\n- Voxelization\\n- Normal estimation\\n- Keypoint detection', \n",
    "        ha='center', va='center', fontsize=9, transform=ax.transAxes,\n",
    "        bbox=dict(boxstyle=\"round,pad=0.3\", facecolor=\"lightyellow\"))\n",
    "\n",
    "ax.text(0.8, 0.5, 'Applications:\\n- Autonomous driving\\n- Robotics\\n- AR/VR\\n- Cultural heritage', \n",
    "        ha='center', va='center', fontsize=9, transform=ax.transAxes,\n",
    "        bbox=dict(boxstyle=\"round,pad=0.3\", facecolor=\"lightcoral\"))\n",
    "\n",
    "ax.set_title('Point Cloud Processing')\n",
    "ax.axis('off')\n",
    "plt.tight_layout()\n",
    "plt.show()\n",
    "\n",
    "print(\"Point Cloud Processing Libraries:\")\n",
    "print(\"1. Open3D - Open-source 3D data processing\")\n",
    "print(\"2. PCL (Point Cloud Library) - Comprehensive 3D library\")\n",
    "print(\"3. PyTorch3D - Deep learning for 3D data\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3. Medical Image Analysis\n",
    "\n",
    "### Medical Imaging Modalities\n",
    "\n",
    "Different imaging techniques provide various types of medical data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Medical imaging modalities\n",
    "fig, ax = plt.subplots(1, 1, figsize=(14, 10))\n",
    "ax.text(0.5, 0.95, 'Medical Imaging Modalities', ha='center', va='center', fontsize=16, transform=ax.transAxes)\n",
    "\n",
    "# Modalities\n",
    "modalities = [
    "    (0.2, 0.8, 'X-Ray\\n- Projection imaging\\n- Bone visualization'),\n",
    "    (0.4, 0.8, 'CT\\n- Cross-sectional\\n- High resolution'),\n",
    "    (0.6, 0.8, 'MRI\\n- Soft tissue\\n- Multi-contrast'),\n",
    "    (0.8, 0.8, 'Ultrasound\\n- Real-time\\n- Safe, portable')\n",
    "]\n",
    "\n",
    "# Draw modalities\n",
    "for x, y, text in modalities:\n",
    "    ax.text(x, y, text, ha='center', va='center', fontsize=9, transform=ax.transAxes,\n",
    "            bbox=dict(boxstyle=\"round,pad=0.3\", facecolor=\"lightblue\"))\n",
    "\n",
    "# Applications\n",
    "applications = [
    "    (0.2, 0.6, 'Segmentation:\\n- Organ delineation\\n- Tumor detection\\n- Lesion quantification'),\n",
    "    (0.5, 0.6, 'Classification:\\n- Disease diagnosis\\n- Treatment response\\n- Prognosis prediction'),\n",
    "    (0.8, 0.6, 'Registration:\\n- Multi-modal fusion\\n- Longitudinal studies\\n- Image-guided surgery')\n",
    "]\n",
    "\n",
    "# Draw applications\n",
    "for x, y, text in applications:\n",
    "    ax.text(x, y, text, ha='center', va='center', fontsize=9, transform=ax.transAxes,\n",
    "            bbox=dict(boxstyle=\"round,pad=0.3\", facecolor=\"lightgreen\"))\n",
    "\n",
    "# Deep learning in medical imaging\n",
    "ax.text(0.5, 0.4, 'Deep Learning in Medical Imaging:\\n- U-Net for segmentation\\n- CNNs for classification\\n- GANs for data augmentation\\n- Transformers for attention', \n",
    "        ha='center', va='center', fontsize=10, transform=ax.transAxes,\n",
    "        bbox=dict(boxstyle=\"round,pad=0.3\", facecolor=\"lightcoral\"))\n",
    "\n",
    "# Challenges\n",
    "ax.text(0.5, 0.2, 'Challenges:\\n- Limited annotated data\\n- Inter-observer variability\\n- Regulatory approval\\n- Interpretability requirements', \n",
    "        ha='center', va='center', fontsize=10, transform=ax.transAxes,\n",
    "        bbox=dict(boxstyle=\"round,pad=0.3\", facecolor=\"lightyellow\"))\n",
    "\n",
    "ax.set_title('Medical Image Analysis')\n",
    "ax.axis('off')\n",
    "plt.tight_layout()\n",
    "plt.show()\n",
    "\n",
    "print(\"Medical Imaging Datasets:\")\n",
    "print(\"1. ISIC - Skin cancer classification\")\n",
    "print(\"2. ChestX-ray8 - Chest disease detection\")\n",
    "print(\"3. BraTS - Brain tumor segmentation\")\n",
    "print(\"4. MIMIC-CXR - Large chest X-ray database\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4. Autonomous Vehicles and Robotics Vision\n",
    "\n",
    "### Perception Pipeline\n",
    "\n",
    "Autonomous systems require robust perception capabilities."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Autonomous vehicle perception pipeline\n",
    "fig, ax = plt.subplots(1, 1, figsize=(16, 12))\n",
    "ax.text(0.5, 0.98, 'Autonomous Vehicle Perception Pipeline', ha='center', va='center', fontsize=16, transform=ax.transAxes)\n",
    "\n",
    "# Input sensors\n",
    "sensors = [
    "    (0.1, 0.8, 'Cameras\\n(RGB)'),\n",
    "    (0.2, 0.8, 'LiDAR\\n(3D Point Cloud)'),\n",
    "    (0.3, 0.8, 'Radar\\n(Distance, Velocity)'),\n",
    "    (0.4, 0.8, 'GPS/IMU\\n(Position, Orientation)')\n",
    "]\n",
    "\n",
    "# Draw sensors\n",
    "for x, y, text in sensors:\n",
    "    ax.text(x, y, text, ha='center', va='center', fontsize=9, transform=ax.transAxes,\n",
    "            bbox=dict(boxstyle=\"round,pad=0.3\", facecolor=\"lightblue\"))\n",
    "\n",
    "# Sensor fusion\n",
    "ax.text(0.25, 0.7, 'Sensor Fusion\\n(Kalman Filter,\\nParticle Filter)', \n",
    "        ha='center', va='center', fontsize=9, transform=ax.transAxes,\n",
    "        bbox=dict(boxstyle=\"round,pad=0.3\", facecolor=\"lightgreen\"))\n",
    "\n",
    "# Perception modules\n",
    "modules = [
    "    (0.5, 0.8, 'Object Detection\\n(YOLO, R-CNN)'),\n",
    "    (0.6, 0.8, 'Object Tracking\\n(SORT, DeepSORT)'),\n",
    "    (0.7, 0.8, 'Semantic Segmentation\\n(DeepLab, PSPNet)'),\n",
    "    (0.8, 0.8, 'Free Space\\nDetection')\n",
    "]\n",
    "\n",
    "# Draw modules\n",
    "for x, y, text in modules:\n",
    "    ax.text(x, y, text, ha='center', va='center', fontsize=9, transform=ax.transAxes,\n",
    "            bbox=dict(boxstyle=\"round,pad=0.3\", facecolor=\"lightyellow\"))\n",
    "\n",
    "# Scene understanding\n",
    "ax.text(0.65, 0.7, 'Scene Understanding\\n(Static/Dynamic,\\nDrivable area)', \n",
    "        ha='center', va='center', fontsize=9, transform=ax.transAxes,\n",
    "        bbox=dict(boxstyle=\"round,pad=0.3\", facecolor=\"lightcoral\"))\n",
    "\n",
    "# World model\n",
    "ax.text(0.5, 0.6, 'World Model\\n(3D Map,\\nObject states)', \n",
    "        ha='center', va='center', fontsize=9, transform=ax.transAxes,\n",
    "        bbox=dict(boxstyle=\"round,pad=0.3\", facecolor=\"lightpink\"))\n",
    "\n",
    "# Planning and control\n",
    "ax.text(0.5, 0.5, 'Planning & Control\\n(Path planning,\\nTrajectory generation)', \n",
    "        ha='center', va='center', fontsize=9, transform=ax.transAxes,\n",
    "        bbox=dict(boxstyle=\"round,pad=0.3\", facecolor=\"lightcyan\"))\n",
    "\n",
    "# Connections\n",
    "ax.annotate('', xy=(0.25, 0.75), xytext=(0.15, 0.8), arrowprops=dict(arrowstyle=\"->\", lw=1))\n",
    "ax.annotate('', xy=(0.25, 0.75), xytext=(0.25, 0.8), arrowprops=dict(arrowstyle=\"->\", lw=1))\n",
    "ax.annotate('', xy=(0.25, 0.75), xytext=(0.35, 0.8), arrowprops=dict(arrowstyle=\"->\", lw=1))\n",
    "ax.annotate('', xy=(0.65, 0.75), xytext=(0.55, 0.8), arrowprops=dict(arrowstyle=\"->\", lw=1))\n",
    "ax.annotate('', xy=(0.65, 0.75), xytext=(0.65, 0.8), arrowprops=dict(arrowstyle=\"->\", lw=1))\n",
    "ax.annotate('', xy=(0.65, 0.75), xytext=(0.75, 0.8), arrowprops=dict(arrowstyle=\"->\", lw=1))\n",
    "ax.annotate('', xy=(0.5, 0.65), xytext=(0.25, 0.7), arrowprops=dict(arrowstyle=\"->\", lw=1))\n",
    "ax.annotate('', xy=(0.5, 0.65), xytext=(0.65, 0.7), arrowprops=dict(arrowstyle=\"->\", lw=1))\n",
    "ax.annotate('', xy=(0.5, 0.55), xytext=(0.5, 0.6), arrowprops=dict(arrowstyle=\"->\", lw=1))\n",
    "\n",
    "# Challenges and technologies\n",
    "ax.text(0.25, 0.3, 'Key Technologies:\\n- Deep learning\\n- Sensor fusion\\n- Real-time processing\\n- Domain adaptation', \n",
    "        ha='center', va='center', fontsize=10, transform=ax.transAxes,\n",
    "        bbox=dict(boxstyle=\"round,pad=0.3\", facecolor=\"lightgray\"))\n",
    "\n",
    "ax.text(0.75, 0.3, 'Challenges:\\n- Adverse weather\\n- Occlusions\\n- Rare scenarios\\n- Safety validation', \n",
    "        ha='center', va='center', fontsize=10, transform=ax.transAxes,\n",
    "        bbox=dict(boxstyle=\"round,pad=0.3\", facecolor=\"lightcoral\"))\n",
    "\n",
    "ax.set_title('Autonomous Vehicle Perception')\n",
    "ax.axis('off')\n",
    "plt.tight_layout()\n",
    "plt.show()\n",
    "\n",
    "print(\"Autonomous Vehicle Datasets:\")\n",
    "print(\"1. KITTI - Multi-modal dataset for autonomous driving\")\n",
    "print(\"2. nuScenes - Large-scale autonomous driving dataset\")\n",
    "print(\"3. Waymo Open Dataset - High-quality sensor data\")\n",
    "print(\"4. ApolloScape - Large-scale autonomous driving dataset\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 5. Augmented Reality Applications\n",
    "\n",
    "### AR Pipeline\n",
    "\n",
    "Augmented reality overlays digital information on the real world."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Augmented reality pipeline\n",
    "fig, ax = plt.subplots(1, 1, figsize=(14, 10))\n",
    "ax.text(0.5, 0.95, 'Augmented Reality Pipeline', ha='center', va='center', fontsize=16, transform=ax.transAxes)\n",
    "\n",
    "# AR pipeline steps\n",
    "steps = [
    "    (0.1, 0.8, 'Input\\n(Camera)'),\n",
    "    (0.25, 0.8, 'Tracking\\n(SLAM)'),\n",
    "    (0.4, 0.8, 'Mapping\\n(3D Reconstruction)'),\n",
    "    (0.55, 0.8, 'Understanding\\n(Object Recognition)'),\n",
    "    (0.7, 0.8, 'Rendering\\n(Graphics)'),\n",
    "    (0.85, 0.8, 'Output\\n(Display)')\n",
    "]\n",
    "\n",
    "# Draw steps\n",
    "for i, (x, y, text) in enumerate(steps):\n",
    "    ax.text(x, y, text, ha='center', va='center', fontsize=9, transform=ax.transAxes,\n",
    "            bbox=dict(boxstyle=\"round,pad=0.3\", facecolor=\"lightblue\"))\n",
    "    if i > 0:\n",
    "        ax.annotate('', xy=(x, y), xytext=(steps[i-1][0], steps[i-1][1]), arrowprops=dict(arrowstyle=\"->\", lw=2))\n",
    "\n",
    "# SLAM (Simultaneous Localization and Mapping)\n",
    "ax.text(0.25, 0.6, 'SLAM:\\n- Visual SLAM\\n- VIO (Visual-Inertial)\\n- Feature-based / Direct methods', \n",
    "        ha='center', va='center', fontsize=9, transform=ax.transAxes,\n",
    "        bbox=dict(boxstyle=\"round,pad=0.3\", facecolor=\"lightgreen\"))\n",
    "\n",
    "# AR Applications\n",
    "applications = [
    "    (0.2, 0.4, 'Gaming:\\n- Pokemon GO\\n- Minecraft Earth'),\n",
    "    (0.4, 0.4, 'Education:\\n- Anatomy visualization\\n- Historical recreation'),\n",
    "    (0.6, 0.4, 'Retail:\\n- Virtual try-on\\n- Furniture placement'),\n",
    "    (0.8, 0.4, 'Industrial:\\n- Maintenance assistance\\n- Training simulations')\n",
    "]\n",
    "\n",
    "# Draw applications\n",
    "for x, y, text in applications:\n",
    "    ax.text(x, y, text, ha='center', va='center', fontsize=9, transform=ax.transAxes,\n",
    "            bbox=dict(boxstyle=\"round,pad=0.3\", facecolor=\"lightcoral\"))\n",
    "\n",
    "# Technologies\n",
    "ax.text(0.5, 0.2, 'Key Technologies:\\n- Computer vision\\n- 3D reconstruction\\n- Real-time rendering\\n- Mobile optimization', \n",
    "        ha='center', va='center', fontsize=10, transform=ax.transAxes,\n",
    "        bbox=dict(boxstyle=\"round,pad=0.3\", facecolor=\"lightyellow\"))\n",
    "\n",
    "ax.set_title('Augmented Reality System')\n",
    "ax.axis('off')\n",
    "plt.tight_layout()\n",
    "plt.show()\n",
    "\n",
    "print(\"AR Development Frameworks:\")\n",
    "print(\"1. ARKit (Apple) - iOS AR development\")\n",
    "print(\"2. ARCore (Google) - Android AR development\")\n",
    "print(\"3. Unity AR Foundation - Cross-platform AR\")\n",
    "print(\"4. OpenCV - Computer vision for AR\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Summary\n",
    "\n",
    "In this final module, we've explored specialized applications of computer vision:\n",
    "1. Video analysis and tracking for motion detection and object tracking\n",
    "2. 3D computer vision with point cloud processing\n",
    "3. Medical image analysis for healthcare applications\n",
    "4. Autonomous vehicle perception systems\n",
    "5. Augmented reality applications\n",
    "\n",
    "These specialized domains demonstrate the wide-ranging impact of computer vision across industries. Each application area has unique challenges and requirements that drive innovation in the field.\n",
    "\n",
    "Congratulations on completing this comprehensive computer vision learning path! You now have a solid foundation in both classical and modern computer vision techniques, from basic image processing to advanced deep learning applications."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}