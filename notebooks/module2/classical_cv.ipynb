{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Module 2: Classical Computer Vision Techniques\n",
    "\n",
    "## Learning Objectives\n",
    "By the end of this module, you should be able to:\n",
    "- Detect and describe key features in images\n",
    "- Match features between images\n",
    "- Apply geometric transformations\n",
    "- Create panorama images from multiple photos\n",
    "- Detect objects using classical methods\n",
    "\n",
    "## Topics Covered\n",
    "- Feature detection and description (SIFT, SURF, ORB)\n",
    "- Feature matching\n",
    "- Image transformations (Affine, Perspective)\n",
    "- Image stitching and panorama creation\n",
    "- Object detection with Haar cascades\n",
    "- Camera calibration and stereo vision"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1. Feature Detection and Description\n",
    "\n",
    "Features are distinctive points in an image that can be reliably detected and matched across different views of the same scene.\n",
    "\n",
    "### ORB (Oriented FAST and Rotated BRIEF)\n",
    "\n",
    "ORB is a fast and free alternative to SIFT and SURF."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import cv2\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "# Load images\n",
    "img1 = cv2.imread('../data/sample1.jpg', 0)  # queryImage\n",
    "img2 = cv2.imread('../data/sample2.jpg', 0)  # trainImage\n",
    "\n",
    "# Check if images were loaded\n",
    "if img1 is None or img2 is None:\n",
    "    # Create sample images if data files don't exist\n",
    "    img1 = np.random.randint(0, 255, (500, 500), dtype=np.uint8)\n",
    "    img2 = np.random.randint(0, 255, (500, 500), dtype=np.uint8)\n",
    "    # Add some shapes to make them different\n",
    "    cv2.rectangle(img1, (100, 100), (300, 300), 255, -1)\n",
    "    cv2.circle(img2, (250, 250), 100, 255, -1)\n",
    "\n",
    "# Initialize ORB detector\n",
    "orb = cv2.ORB_create()\n",
    "\n",
    "# Find keypoints and descriptors\n",
    "kp1, des1 = orb.detectAndCompute(img1, None)\n",
    "kp2, des2 = orb.detectAndCompute(img2, None)\n",
    "\n",
    "# Draw keypoints\n",
    "img1_kp = cv2.drawKeypoints(img1, kp1, None, color=(0,255,0), flags=0)\n",
    "img2_kp = cv2.drawKeypoints(img2, kp2, None, color=(0,255,0), flags=0)\n",
    "\n",
    "fig, axes = plt.subplots(1, 2, figsize=(15, 7))\n",
    "axes[0].imshow(img1_kp, cmap='gray')\n",
    "axes[0].set_title(f'Image 1 - {len(kp1)} keypoints')\n",
    "axes[0].axis('off')\n",
    "\n",
    "axes[1].imshow(img2_kp, cmap='gray')\n",
    "axes[1].set_title(f'Image 2 - {len(kp2)} keypoints')\n",
    "axes[1].axis('off')\n",
    "\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2. Feature Matching\n",
    "\n",
    "After detecting features, we need to match them between images."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create BFMatcher object\n",
    "bf = cv2.BFMatcher(cv2.NORM_HAMMING, crossCheck=True)\n",
    "\n",
    "# Match descriptors\n",
    "matches = bf.match(des1, des2)\n",
    "\n",
    "# Sort matches by distance\n",
    "matches = sorted(matches, key=lambda x: x.distance)\n",
    "\n",
    "# Draw first 10 matches\n",
    "img_matches = cv2.drawMatches(img1, kp1, img2, kp2, matches[:10], None, flags=cv2.DrawMatchesFlags_NOT_DRAW_SINGLE_POINTS)\n",
    "\n",
    "plt.figure(figsize=(15, 10))\n",
    "plt.imshow(img_matches)\n",
    "plt.title(f'Feature Matches - Top 10 (Total: {len(matches)})')\n",
    "plt.axis('off')\n",
    "plt.show()\n",
    "\n",
    "print(f\"Number of matches: {len(matches)}\")\n",
    "if len(matches) > 0:\n",
    "    print(f\"Best match distance: {matches[0].distance}\")\n",
    "    print(f\"Worst match distance: {matches[-1].distance}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3. Image Transformations\n",
    "\n",
    "### Affine Transformation\n",
    "\n",
    "Affine transformations preserve lines and parallelism."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define three points in the original image\n",
    "pts1 = np.float32([[50, 50], [200, 50], [50, 200]])\n",
    "\n",
    "# Define corresponding points in the transformed image\n",
    "pts2 = np.float32([[10, 100], [200, 50], [100, 250]])\n",
    "\n",
    "# Get the affine transformation matrix\n",
    "M = cv2.getAffineTransform(pts1, pts2)\n",
    "\n",
    "# Apply the transformation\n",
    "img_affine = cv2.warpAffine(img1, M, (img1.shape[1], img1.shape[0]))\n",
    "\n",
    "fig, axes = plt.subplots(1, 2, figsize=(12, 6))\n",
    "axes[0].imshow(img1, cmap='gray')\n",
    "axes[0].plot(pts1[:, 0], pts1[:, 1], 'ro')\n",
    "axes[0].set_title('Original Image')\n",
    "axes[0].axis('off')\n",
    "\n",
    "axes[1].imshow(img_affine, cmap='gray')\n",
    "axes[1].plot(pts2[:, 0], pts2[:, 1], 'ro')\n",
    "axes[1].set_title('Affine Transformed Image')\n",
    "axes[1].axis('off')\n",
    "\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Perspective Transformation\n",
    "\n",
    "Perspective transformations can simulate viewpoint changes."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define four points in the original image\n",
    "pts1 = np.float32([[56, 65], [368, 52], [28, 387], [389, 390]])\n",
    "\n",
    "# Define corresponding points in the transformed image\n",
    "pts2 = np.float32([[0, 0], [300, 0], [0, 300], [300, 300]])\n",
    "\n",
    "# Get the perspective transformation matrix\n",
    "M = cv2.getPerspectiveTransform(pts1, pts2)\n",
    "\n",
    "# Apply the transformation\n",
    "img_perspective = cv2.warpPerspective(img1, M, (300, 300))\n",
    "\n",
    "fig, axes = plt.subplots(1, 2, figsize=(12, 6))\n",
    "axes[0].imshow(img1, cmap='gray')\n",
    "axes[0].plot(pts1[:, 0], pts1[:, 1], 'ro')\n",
    "axes[0].set_title('Original Image')\n",
    "axes[0].axis('off')\n",
    "\n",
    "axes[1].imshow(img_perspective, cmap='gray')\n",
    "axes[1].plot(pts2[:, 0], pts2[:, 1], 'ro')\n",
    "axes[1].set_title('Perspective Transformed Image')\n",
    "axes[1].axis('off')\n",
    "\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4. Image Stitching and Panorama Creation\n",
    "\n",
    "Let's create a simple panorama using feature matching and homography."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# For panorama creation, we need to find homography between images\n",
    "if len(matches) > 4:  # Need at least 4 matches for homography\n",
    "    # Extract matched keypoints\n",
    "    src_pts = np.float32([kp1[m.queryIdx].pt for m in matches]).reshape(-1, 1, 2)\n",
    "    dst_pts = np.float32([kp2[m.trainIdx].pt for m in matches]).reshape(-1, 1, 2)\n",
    "    \n",
    "    # Find homography matrix\n",
    "    M, mask = cv2.findHomography(src_pts, dst_pts, cv2.RANSAC, 5.0)\n",
    "    \n",
    "    if M is not None:\n",
    "        # Warp img1 to img2's perspective\n",
    "        result = cv2.warpPerspective(img1, M, (img1.shape[1] + img2.shape[1], img1.shape[0]))\n",
    "        result[0:img2.shape[0], 0:img2.shape[1]] = img2\n",
    "        \n",
    "        plt.figure(figsize=(15, 10))\n",
    "        plt.imshow(result, cmap='gray')\n",
    "        plt.title('Simple Panorama')\n",
    "        plt.axis('off')\n",
    "        plt.show()\n",
    "    else:\n",
    "        print(\"Homography could not be computed\")\n",
    "else:\n",
    "    print(\"Not enough matches for homography computation\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 5. Object Detection with Haar Cascades\n",
    "\n",
    "Haar cascades are machine learning-based approaches for object detection."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load the cascade classifier for face detection\n",
    "# Note: This requires the Haar cascade file which may not be available\n",
    "try:\n",
    "    face_cascade = cv2.CascadeClassifier(cv2.data.haarcascades + 'haarcascade_frontalface_default.xml')\n",
    "    \n",
    "    # For demonstration, let's create a sample image with a \"face-like\" pattern\n",
    "    sample_img = np.zeros((300, 300), dtype=np.uint8)\n",
    "    # Create a simple face-like pattern\n",
    "    cv2.circle(sample_img, (150, 150), 80, 255, -1)  # Face\n",
    "    cv2.circle(sample_img, (120, 130), 15, 0, -1)    # Left eye\n",
    "    cv2.circle(sample_img, (180, 130), 15, 0, -1)    # Right eye\n",
    "    cv2.ellipse(sample_img, (150, 200), (30, 15), 0, 0, 180, 0, -1)  # Smile\n",
    "    \n",
    "    # Detect faces\n",
    "    faces = face_cascade.detectMultiScale(sample_img, 1.1, 4)\n",
    "    \n",
    "    # Draw rectangles around faces\n",
    "    img_faces = sample_img.copy()\n",
    "    for (x, y, w, h) in faces:\n",
    "        cv2.rectangle(img_faces, (x, y), (x+w, y+h), (255, 255, 255), 2)\n",
    "    \n",
    "    fig, axes = plt.subplots(1, 2, figsize=(12, 6))\n",
    "    axes[0].imshow(sample_img, cmap='gray')\n",
    "    axes[0].set_title('Original Image')\n",
    "    axes[0].axis('off')\n",
    "    \n",
    "    axes[1].imshow(img_faces, cmap='gray')\n",
    "    axes[1].set_title(f'Faces Detected ({len(faces)} found)')\n",
    "    axes[1].axis('off')\n",
    "    \n",
    "    plt.show()\n",
    "    \n",
    "    print(f\"Number of faces detected: {len(faces)}\")\n",
    "except:\n",
    "    print(\"Haar cascade file not found. Skipping face detection demo.\")\n",
    "    \n",
    "    # Show a conceptual example\n",
    "    fig, ax = plt.subplots(1, 1, figsize=(8, 6))\n",
    "    ax.text(0.5, 0.5, 'Haar Cascade Face Detection\\n\\nIn practice, this would detect faces\\nin real images using pre-trained models', \n",
    "            ha='center', va='center', fontsize=14, transform=ax.transAxes)\n",
    "    ax.set_title('Conceptual Example')\n",
    "    ax.axis('off')\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 6. Camera Calibration\n",
    "\n",
    "Camera calibration is the process of determining the intrinsic and extrinsic parameters of a camera."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Conceptual example of camera calibration\n",
    "\n",
    "# In practice, you would use a chessboard pattern and cv2.findChessboardCorners\n",
    "# Here's a conceptual representation:\n",
    "\n",
    "fig, ax = plt.subplots(1, 1, figsize=(10, 8))\n",
    "ax.text(0.5, 0.7, 'Camera Calibration Process', ha='center', va='center', fontsize=16, transform=ax.transAxes)\n",
    "\n",
    "ax.text(0.3, 0.5, '1. Capture images of\\nchessboard pattern\\nfrom different angles', ha='center', va='center', fontsize=12, transform=ax.transAxes)\n",
    "ax.text(0.7, 0.5, '2. Detect corners\\nusing cv2.findChessboardCorners()', ha='center', va='center', fontsize=12, transform=ax.transAxes)\n",
    "ax.text(0.3, 0.3, '3. Compute camera\\nmatrix with\\ncv2.calibrateCamera()', ha='center', va='center', fontsize=12, transform=ax.transAxes)\n",
    "ax.text(0.7, 0.3, '4. Undistort images\\nusing camera parameters', ha='center', va='center', fontsize=12, transform=ax.transAxes)\n",
    "\n",
    "ax.set_title('Conceptual Overview')\n",
    "ax.axis('off')\n",
    "plt.show()\n",
    "\n",
    "print(\"Camera calibration involves these steps:\")\n",
    "print(\"1. Capture multiple images of a known pattern (like a chessboard)\")\n",
    "print(\"2. Detect the corners of the pattern in each image\")\n",
    "print(\"3. Use these correspondences to compute intrinsic and extrinsic parameters\")\n",
    "print(\"4. Apply the calibration to correct for lens distortion\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Summary\n",
    "\n",
    "In this module, we've covered:\n",
    "1. Feature detection and description using ORB\n",
    "2. Feature matching between images\n",
    "3. Geometric transformations (Affine and Perspective)\n",
    "4. Image stitching and panorama creation\n",
    "5. Object detection with Haar cascades\n",
    "6. Camera calibration concepts\n",
    "\n",
    "These classical computer vision techniques form the foundation for many applications and provide important context for understanding modern deep learning approaches. In the next module, we'll explore how traditional machine learning algorithms can be applied to computer vision tasks."
   ]
  }
 ],
 \"metadata\": {
  \"kernelspec\": {
   \"display_name\": \"Python 3\",
   \"language\": \"python\",
   \"name\": \"python3\"
  },
  \"language_info\": {
   \"codemirror_mode\": {
    \"name\": \"ipython\",
    \"version\": 3
   },
   \"file_extension\": \".py\",
   \"mimetype\": \"text/x-python\",
   \"name\": \"python\",
   \"nbconvert_exporter\": \"python\",
   \"pygments_lexer\": \"ipython3\",
   \"version\": \"3.8.5\"
  }
 },
 \"nbformat\": 4,
 \"nbformat_minor\": 4
}